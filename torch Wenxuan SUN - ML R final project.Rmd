---
title: "ML R final project"
author: "Wenxuan Sun"
date: "07/06/2021"
output: html_document
---

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
library(tidyverse)                          # Loading THE package/environment
library(naniar)                             # Missing data exploration
library(visdat)                             # Missing data visualization
library(tseries)                            # Great package for time-series
library(lubridate)
library(zoo)
load("data_full.RData")
```

Get min and max date
```{r}
min_date = min(data_full$Date)
max_date = max(data_full$Date)
```

## 1. Deal with missing values

Visualize the missing values
```{r, message = F, warning = F, cache = T, fig.align='center'}
gg_miss_var(data_full)                      # Proportion of missing points per column
vis_dat(data_full, warn_large_data = F)     # Visualization of data types per column
```

The two most affected columns are indeed *P2B* (price-to-book) and *D2E* (debt-to-equity).   
Let's deal with them using 2 different techniques: **times-series imputation** and **cross-sectional imputation**.   
First, let us have a look closer.
```{r, message = F, warning = F, fig.align='center'}
data_full %>% filter(is.na(P2B))      # Where are the missing P2B ratios?
data_full %>% 
  filter(Tick == "ADX") %>%
  ggplot(aes(x = Date, y = P2B)) + geom_line() + theme_light() + ggtitle("ADX_P2B - Adams Diversified Equity Fund")
data_full %>% 
  filter(Tick == "ADMT") %>%
  ggplot(aes(x = Date, y = P2B)) + geom_line() + theme_light() + ggtitle("ADMT_P2B - ADM TRONICS UNLIMITED INC")
```
For time-series imputation, we will use the *na.locf* function from the zoo package. The missing points were replaced by the last preceding value. But for the situations like stock ADX, if there is no preceding value, the NA values cannot be filled.
```{r, message = F, warning = F, fig.align='center'}
data_full %>% 
  group_by(Tick) %>%                         # Perform the operation for EACH stock
  mutate(P2B = na.locf(P2B, na.rm = F)) %>%  # Replace P2B by imputed P2B
  ungroup() %>%                              # Ungroup
  filter(Tick == "ADX") %>% 
  ggplot(aes(x = Date, y = P2B)) + geom_line() + theme_light() + ggtitle("ADX P2B Original")
```
Apparently this method does not work for certain stocks.

Another way to proceed is to assign a mean or median score, as if the missing value for the company is equal to some "*average*" or representative value. The median is more robust than the mean because it is less affected by outliers.
```{r, message = F, warning = F, fig.align='center'}
data_full <- data_full %>% 
  group_by(Date) %>%                                   # Perform the operation for EACH date
  mutate(med_P2B = median(P2B, na.rm = T)) %>%         # Compute the median P2B
  mutate(med_D2E = median(D2E, na.rm = T)) %>% 
  mutate(med_MktCap = median(Mkt_Cap, na.rm = T)) %>% 
  mutate(med_Vol1M = median(Vol_1M, na.rm = T)) %>% 
  ungroup() %>% 
  group_by(Tick) %>%                                   # Perform the operation for each stock
  mutate(P2B = if_else(is.na(P2B), med_P2B, P2B)) %>%  # If P2B is NA, replace it! If not, don't.
  mutate(D2E = if_else(is.na(D2E), med_D2E, D2E)) %>%
  mutate(Mkt_Cap = if_else(is.na(Mkt_Cap), med_MktCap, Mkt_Cap)) %>%
  mutate(Vol_1M = if_else(is.na(Vol_1M), med_Vol1M, Vol_1M)) %>%
  select(-med_MktCap, -med_P2B, -med_D2E, -med_Vol1M) %>% 
  ungroup()

data_full %>% 
  filter(Tick == "ADX") %>% 
  ggplot(aes(x = Date, y = P2B)) + geom_line() + theme_light() + ggtitle("ADX_P2B - Adams Diversified Equity Fund")
```

## 2. Outlier detection (Winsorisation)
$$\tilde{x_i}=\begin{cases}x_i,&x^{(q)}≤x_i≤x^{(1-q)} \\ x^{(q)},&x<x^{(q)} \\ x^{(1-q)}&x_i<x^{(1-q)}\end{cases}$$
Winsorisation for P2B, D2E, Vol_1M, Mkt_Cap columns
```{r}
# winsorisation function
winsor <- function(colu){
  # the q parameter is set as 0.01
  colu = case_when(
    colu >= unname(quantile(colu, probs = 0.01)) & colu <= unname(quantile(colu, probs = 0.99)) ~ colu,
    colu >= unname(quantile(colu, probs = 0.99)) ~ unname(quantile(colu, probs = 0.99)),
    colu <= unname(quantile(colu, probs = 0.01)) ~ unname(quantile(colu, probs = 0.01))
    )
  return(colu)
}

data_full <- data_full %>% 
  group_by(Date) %>% 
  mutate_at(vars(P2B, D2E, Vol_1M, Mkt_Cap), winsor) %>% 
  ungroup()

rm(winsor)
```

## 3. Build an index
Set up the index table (benchmark)
The index is built with 100 USD for each stock at the beginning date 1995-01-31.
The index at the first date is 92,800 USD and ends at 2,475,168.2 USD on 2021-02-26.
```{r}
data_full <- data_full %>% arrange(Date)  # arrange dates
data_full$Date <- as.Date(data_full$Date, origin="1970-01-01")

min_date <- min(data_full$Date)  # begin date
max_date <- max(data_full$Date)  # end date

c100 <- rep(100, length(data_full[data_full$Date == min_date, ]$Close))  # create a vector with length of 928 filling with 100

close_1 <- data_full[data_full$Date == min_date, ]$Close # close price of 928 stocks at the beginning date
Position <- c100 / close_1  # positions for 928 stocks

# index = position * price
index_1 <- t(Position) %*% close_1  # index at the beginning

data_index <- data.frame(Date=character(), Index=numeric())  # dataframe of the index

for(i in unique(data_full$Date)){
  i <- as.Date(i, origin="1970-01-01")
  close_i <- data_full[data_full$Date == i, ]$Close
  index_i <- t(Position) %*% close_i
  data_index_i <- data.frame(Date=i, Index=index_i)
  data_index <- rbind(data_index, data_index_i)
}

# Add a return column in data_index
data_index <- data_index %>%
  mutate(Index_Return = Index / lag(Index) - 1)

# remove variables that are useless later
rm(c100)
rm(close_1)
rm(close_i)
rm(i)
rm(Position)
rm(index_i)
rm(index_1)
rm(data_index_i)

# fill in the first return data with median
data_index$Index_Return[1] <- median(data_index$Index_Return, na.rm = T)
print(head(data_index))

data_index %>% 
  ggplot(aes(x = Date, y = Index)) +
  geom_line() +
  ggtitle("Index Value")

data_index %>% 
  ggplot(aes(x = Date, y = Index_Return)) +
  geom_line() +
  ggtitle("Index Return")
```

## 4. Time series testing
Stationary Testing
```{r stationarity testing, warning=F}
# library(tseries)  # https://cran.r-project.org/web/packages/tseries/tseries.pdf
# Augmented Dickey-Fuller Test with null hypothesis: x is a non-stationary time series
adf_stationary <- function(x){
  # x must be a vector
  if(adf.test(x)["p.value"] < 0.05){  # the confidence interval is set as 95%
    print(paste("Under ADF test,",deparse(substitute(x)),"is stationary"))
  } else {
    print(paste("Under ADF test,",deparse(substitute(x)),"is not stationary"))
  }
}
adf_stationary(data_index$Index)
adf_stationary(data_index$Index_Return)

# Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test with the null hypothesis: x is level or trend stationary
kpss_stationary <- function(x){
  # x must be a vector
  if(kpss.test(x)["p.value"] < 0.05){  # the confidence interval is set as 95%
    print(paste("Under KPSS test,",deparse(substitute(x)),"is not stationary"))
  } else {
    print(paste("Under KPSS test,",deparse(substitute(x)),"is stationary"))
  }
}
kpss_stationary(data_index$Index)
kpss_stationary(data_index$Index_Return)
rm(adf_stationary)
rm(kpss_stationary)
```
Autocorrelation Testing (Ljung-Box Q Test)
```{r autocorrelation testing, warning=F}
# null hypothesis for LBQ test: no serial correlation under the given lag
LBQ_acrr <- function(x, lag){
  # x must be a vector
  if(Box.test(x, lag = lag, type = "Ljung-Box")["p.value"] < 0.05){  # the confidence interval is set as 95%
    print(paste("Under Ljung-Box Q test,",
                deparse(substitute(x)),
                "is autocorrelated under lag",
                deparse(substitute(lag)))
                )
  } else {
    print(paste("Under Ljung-Box Q test,",
                deparse(substitute(x)),
                "is not autocorrelated under lag",
                deparse(substitute(lag)))
                )
  }
}
LBQ_acrr(data_index$Index, lag = 10)
LBQ_acrr(data_index$Index_Return, lag = 10)
rm(LBQ_acrr)
```

## 5. Apply torch algorithm
For the input dataset of the model, the first dimension of input tensor (torch tensor: can be directly transformed from dataframe or matrix) is always the batch size. So, we need to determine training dataset, validation dataset, and test dataset first.

For the input dataset of the model, the first dimension of input tensor (torch tensor: can be directly transformed from dataframe or matrix) is always the batch size.
```{r}
library(torch)
n_timesteps <- 6  # six months
n_forecast <- n_timesteps

ind_train <- data_index %>%   # training dataset
  filter(Date < "2007-04-15") %>% 
  select(Index) %>% 
  as.matrix()
ind_valid <- data_index %>%   # validation dataset
  filter(Date > "2007-04-15" & Date < "2019-07-15") %>% 
  select(Index) %>% 
  as.matrix()
ind_test <- data_index %>%   # testing dataset
  filter(Date > "2019-07-15") %>%
  select(Index) %>% 
  as.matrix()

train_mean <- mean(ind_train)
train_sd <- sd(ind_train)

# set up the function to convert the matrix/dataframe into torch tensor format so that torch can interpret the dataset
prices_dataset <- torch::dataset(
  name = "prices_dataset",  # name of the function
  initialize = function(x, n_timesteps, sample_frac = 1) {
    # make sure there is no NA value in the input dataset, otherwise use
    # df <- na.omit(df) to omit NA values
    self$n_timesteps <- n_timesteps
    self$x <- torch_tensor((x - train_mean) / train_sd)  # standardize the input dataset
    n <- length(self$x) - self$n_timesteps - 1
    self$starts <- sort(sample.int(  # create a sorted list of samples
      n = n,
      size = n * sample_frac  # sample fraction
      )
    )
  },
  .getitem = function(i) {
    start <- self$starts[i]
    end <- start + self$n_timesteps - 1
    lag <- 1
    list(
      x = self$x[start:end],
      y = self$x[(start+lag):(end+lag)]$squeeze(2)
    )
  },
  .length = function() {
    length(self$starts)
  }
)

# dataloader
batch_size <- 16
train_ds <- ind_dataset(ind_train, n_timesteps, sample_frac = 0.8)
train_dl <- train_ds %>% dataloader(batch_size = batch_size, shuffle = TRUE)  # If shuffle is set to True, all the samples are shuffled and loaded in batches. Otherwise they are sent one-by-one without any shuffling
valid_ds <- ind_dataset(ind_valid, n_timesteps, sample_frac = 0.8)
valid_dl <- valid_ds %>% dataloader(batch_size = batch_size)
test_ds <- ind_dataset(ind_test, n_timesteps)
test_dl <- test_ds %>% dataloader(batch_size = 1)  # batch size=1 means that the sample will be updated after each test
```

Because CNN (convolutional neural network) falls short in interpreting the temporal information such as time series. So, for accuracy, I use Seq2seq model that consists of two RNN (recurrent neural networks): encoder and decoder.
```{r encoder}
# The encoder takes its input and runs it through an RNN. Of the two things returned by a recurrent neural network, outputs and state, so far we’ve only been using output. This time, we do the opposite: We throw away the outputs, and only return the state.
encoder_module <- nn_module(
  initialize = function(type, input_size, hidden_size, num_layers = 1, dropout = 0) {
    self$type <- type
    self$rnn <- if (self$type == "gru") {
      nn_gru(  # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html
        input_size = input_size,  # The number of expected features in the input x
        hidden_size = hidden_size,  # The number of features in the hidden state h
        num_layers = num_layers,
        dropout = dropout,  # dropout probability = dropout, when dropout≠0 it will be executed
        batch_first = TRUE  # If True, then the input and output tensors are provided as (batch, seq, feature)
      )
    } else {
      nn_lstm(  # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html
        input_size = input_size,
        hidden_size = hidden_size,
        num_layers = num_layers,
        dropout = dropout,
        batch_first = TRUE
      )
    }
  },
  forward = function(x) {
    x <- self$rnn(x)
    # return last states for all layers
    # per layer, a single tensor for GRU, a list of 2 tensors for LSTM
    x <- x[[2]]
    x
  }
)
```


```{r decoder module}
# In the decoder, just like in the encoder, the main component is an RNN. It returns a prediction.
decoder_module <- nn_module(
  initialize = function(type, input_size, hidden_size, num_layers = 1) {
    self$type <- type
    self$rnn <- if (self$type == "gru") {
      nn_gru(
        input_size = input_size,
        hidden_size = hidden_size,
        num_layers = num_layers,
        batch_first = TRUE
      )
    } else {
      nn_lstm(
        input_size = input_size,
        hidden_size = hidden_size,
        num_layers = num_layers,
        batch_first = TRUE
      )
    }
    self$linear <- nn_linear(hidden_size, 1)
  },
  forward = function(x, state) {
    # input to forward:
    # x is (batch_size, 1, 1)
    # state is (1, batch_size, hidden_size)
    x <- self$rnn(x)
    # break up RNN return values
    # output is (batch_size, 1, hidden_size)
    # next_hidden is
    c(output, next_hidden) %<-% x
    output <- output$squeeze(2)
    output <- self$linear(output)
    list(output, next_hidden)
  }
)
```


```{r seq2seq module}
seq2seq_module <- nn_module(
  initialize = function(type, input_size, hidden_size, n_forecast, num_layers = 1, encoder_dropout = 0) {
    self$encoder <- encoder_module(type = type, input_size = input_size,
                                   hidden_size = hidden_size, num_layers, encoder_dropout)
    self$decoder <- decoder_module(type = type, input_size = input_size,
                                   hidden_size = hidden_size, num_layers)
    self$n_forecast <- n_forecast
  },
  # With teacher forcing, instead of the forecast we pass the actual ground truth, the thing the decoder should have predicted. We do that only in a configurable fraction of cases, and – naturally – only while training.
  forward = function(x, y, teacher_forcing_ratio) {
    # prepare empty output
    outputs <- torch_zeros(dim(x)[1], self$n_forecast)$to(device = device)
    # encode current input sequence
    hidden <- self$encoder(x)
    # prime decoder with final input value and hidden state from the encoder
    out <- self$decoder(x[ , n_timesteps, , drop = FALSE], hidden)
    # decompose into predictions and decoder state
    # pred is (batch_size, 1)
    # state is (1, batch_size, hidden_size)
    c(pred, state) %<-% out
    # store first prediction
    outputs[ , 1] <- pred$squeeze(2)
    # iterate to generate remaining forecasts
    for (t in 2:self$n_forecast) {
      # call decoder on either ground truth or previous prediction, plus previous decoder state
      teacher_forcing <- runif(1) < teacher_forcing_ratio
      input <- if (teacher_forcing == TRUE) y[ , t - 1, drop = FALSE] else pred
      input <- input$unsqueeze(3)
      out <- self$decoder(input, state)
      # again, decompose decoder return values
      c(pred, state) %<-% out
      # and store current prediction
      outputs[ , t] <- pred$squeeze(2)
    }
    outputs
  }
)
net <- seq2seq_module("gru", input_size = 1, hidden_size = 32, n_forecast = n_forecast)
device <- torch_device("cpu")
net <- net$to(device = device)
```
